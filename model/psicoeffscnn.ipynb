{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backend Selected: gpu\n",
      "Detected Devices: [CudaDevice(id=0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1096587/1712071315.py:18: DeprecationWarning: jax.lib.xla_bridge.get_backend is deprecated; use jax.extend.backend.get_backend.\n",
      "  print(\"Backend Selected:\", jax.lib.xla_bridge.get_backend().platform)\n"
     ]
    }
   ],
   "source": [
    "import flax.linen as nn\n",
    "import jax.numpy as jnp\n",
    "import os\n",
    "import jax\n",
    "from jax import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import enlighten\n",
    "\n",
    "\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "devices = jax.local_devices()\n",
    "\n",
    "print(\"Backend Selected:\", jax.lib.xla_bridge.get_backend().platform)\n",
    "print(\"Detected Devices:\", jax.devices())\n",
    "\n",
    "\n",
    "\n",
    "root_key = jax.random.PRNGKey(seed=0)\n",
    "main_key, params_key, rng_key = jax.random.split(key=root_key, num=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnum_samples = 30\\n\\n# Test sample data\\nsignal_data = [np.random.randn(1000) + 1j * np.random.randn(1000) for _ in range(num_samples)]\\ncoefficients_data = [np.random.randn(6) + 1j * np.random.randn(6) for _ in range(num_samples)]\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#making sample data\n",
    "\"\"\"\n",
    "num_samples = 30\n",
    "\n",
    "# Test sample data\n",
    "signal_data = [np.random.randn(1000) + 1j * np.random.randn(1000) for _ in range(num_samples)]\n",
    "coefficients_data = [np.random.randn(6) + 1j * np.random.randn(6) for _ in range(num_samples)]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1096587/713741733.py:14: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  label_matrix = label_df.applymap(convert_to_complex).to_numpy().T  # Transpose to get data points as rows\n",
      "/tmp/ipykernel_1096587/713741733.py:15: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  data_matrix = data_df.applymap(convert_to_complex).to_numpy().T    # Transpose to get data points as rows\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Matrix Split Shape: (10000, 12)\n",
      "Data Matrix Split Shape: (10000, 2882)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the path to the CSV files\n",
    "label_file_path = '/home/houtlaw/iono-net/data/SAR_AF_ML_toyDataset_etc/radar_coeffs_csv/compl_ampls_20241026_201104.csv'\n",
    "data_file_path = '/home/houtlaw/iono-net/data/SAR_AF_ML_toyDataset_etc/radar_coeffs_csv/nuStruct_withSpeckle_20241026_201052.csv'\n",
    "\n",
    "# Function to convert complex strings (e.g., '5.7618732844527+1.82124094798357i') to complex numbers\n",
    "def convert_to_complex(s):\n",
    "    return complex(s.replace('i', 'j'))\n",
    "\n",
    "# Load the CSV files using pandas and apply conversion to complex numbers\n",
    "label_df = pd.read_csv(label_file_path, dtype=str)\n",
    "data_df = pd.read_csv(data_file_path, dtype=str)\n",
    "\n",
    "# Convert the string representations into complex values\n",
    "label_matrix = label_df.applymap(convert_to_complex).to_numpy().T  # Transpose to get data points as rows\n",
    "data_matrix = data_df.applymap(convert_to_complex).to_numpy().T    # Transpose to get data points as rows\n",
    "\n",
    "\n",
    "# Split complex matrices into real and imaginary parts\n",
    "def split_complex_to_imaginary(complex_array):\n",
    "    return np.concatenate([complex_array.real, complex_array.imag], axis=-1)\n",
    "\n",
    "# Now each row represents a data point with real and imaginary parts concatenated along the row\n",
    "label_matrix_split = split_complex_to_imaginary(label_matrix)\n",
    "data_matrix_split = split_complex_to_imaginary(data_matrix)\n",
    "\n",
    "print(\"Label Matrix Split Shape:\", label_matrix_split.shape)\n",
    "print(\"Data Matrix Split Shape:\", data_matrix_split.shape)\n",
    "\n",
    "# Combine the signal (data) and coefficients (labels) into a dataset\n",
    "# Each signal now has length 2000 (real + imaginary), coefficients have length 12 (real + imaginary)\n",
    "dataset = list(zip(data_matrix_split, label_matrix_split))\n",
    "\n",
    "# Data loader function\n",
    "def data_loader(dataset, batch_size, shuffle=True):\n",
    "    dataset_size = len(dataset)\n",
    "    indices = np.arange(dataset_size)\n",
    "    \n",
    "    # Shuffle dataset if required\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    \n",
    "    # Loop over dataset and yield batches\n",
    "    for start_idx in range(0, dataset_size, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, dataset_size)\n",
    "        batch_indices = indices[start_idx:end_idx]\n",
    "        \n",
    "        # Extract the batch of signals and coefficients separately\n",
    "        batch_signal = [dataset[i][0] for i in batch_indices]  # Signal of length 2000\n",
    "        batch_coefficients = [dataset[i][1] for i in batch_indices]  # Coefficients of length 12\n",
    "        \n",
    "        # Convert the batch data to JAX arrays\n",
    "        yield jnp.array(batch_signal), jnp.array(batch_coefficients)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "num_batches = len(dataset) // batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data config\n",
    "batch_size = 32\n",
    "signal_length = batch_signal.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax.linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "\n",
    "class ComplexFCNN(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x, deterministic, rngs={'dropout': None}):  # Remove rng_key as a default here\n",
    "        # First dense layer: input shape should match the length of the signal (2000)\n",
    "        x = nn.Dense(128)(x)  # First fully connected layer\n",
    "        x = nn.relu(x)\n",
    "        \n",
    "        # Apply dropout after the first layer\n",
    "        x = nn.Dropout(0.2)(x, deterministic=deterministic)\n",
    "        \n",
    "        # Second dense layer\n",
    "        x = nn.Dense(64)(x)   # Second fully connected layer\n",
    "        x = nn.relu(x)\n",
    "        \n",
    "        # Final dense layer to output 12 values (6 real and 6 imaginary values for coefficients)\n",
    "        x = nn.Dense(12)(x)  # Output layer with 12 units\n",
    "        return x\n",
    "\n",
    "# Instance of the model\n",
    "model = ComplexFCNN()\n",
    "\n",
    "# Set seed\n",
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "# Assuming batch_size and signal_length have been defined earlier\n",
    "batch_size = 32\n",
    "signal_length = 1441  # From your transposed data (2000 real+imaginary)\n",
    "\n",
    "# Define input shape: (batch_size, 2000) because of real and imaginary concatenation\n",
    "input_shape = (batch_size, 2 * signal_length)\n",
    "\n",
    "# Initialize model variables with random inputs of ones (for testing initialization)\n",
    "variables = model.init(key, jnp.ones(input_shape), deterministic=True)\n",
    "\n",
    "# Forward pass to test the configuration and output shape\n",
    "output = model.apply(variables, jnp.ones(input_shape), deterministic=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loss_fn(params, model, inputs, true_coeffs, deterministic, rng_key):\n",
    "    # Forward pass with the deterministic flag and PRNG key for dropout\n",
    "    preds = model.apply({'params': params}, inputs, deterministic=deterministic, rngs={'dropout': rng_key})\n",
    "    \n",
    "    # Split predictions into real and imaginary parts\n",
    "    preds_real, preds_imag = preds[:, :6], preds[:, 6:]\n",
    "\n",
    "    true_real, true_imag = true_coeffs[:, :6], true_coeffs[:, 6:]\n",
    "    \n",
    "    # MSE loss for real and imaginary parts\n",
    "    loss_real = jnp.mean((preds_real - true_real) ** 2)\n",
    "    loss_imag = jnp.mean((preds_imag - true_imag) ** 2)\n",
    "    \n",
    "    return loss_real + loss_imag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training config dict\n",
    "tr_config = {\n",
    "    \"lr_0\" : 0.001,              # Initial learning rate for the optimizer\n",
    "    \"lr_gamma\": 0.95,            # Learning rate decay factor (multiplies the learning rate by this value at each step)\n",
    "    \"lr_step\" : 1000,            # Number of iterations after which the learning rate is updated (decayed)\n",
    "    \"lr_f\" : 1e-5,               # Final learning rate (smallest allowed learning rate after decay)\n",
    "    \"maxiter_adam\" : 10000,       # Maximum number of iterations for the Adam optimizer\n",
    "    \"maxiter_lbfgs\": 1000,       # Maximum number of iterations for the L-BFGS optimizer\n",
    "    \"deepOnet_width\" : 12,       # Width of the hidden layers in the DeepONet model (number of neurons per layer)\n",
    "    \"trunk_architecture\" : [50, 30, 10],  # Architecture of the trunk network, with 3 layers each containing 50 neurons\n",
    "    \"trunk_activation\": jnp.tanh, # Activation function used in the trunk network (tanh in this case)\n",
    "    \"trunk_input_dim\": 1,        # Input dimension for the trunk network\n",
    "    \"trunk_output_dim\": 1,       # Output dimension for the trunk network\n",
    "    \"trunk_sensor\": 2881,         # Number of sensors (inputs) for the trunk network\n",
    "    \"num_train\": 1000,             # Number of training samples\n",
    "    \"num_test\": 300                # Number of test samples\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".enlighten-fg-red {\n",
       "  color: #cd0000;\n",
       "}\n",
       "</style>\n",
       "<div class=\"enlighten\">\n",
       "  <div class=\"enlighten-bar\">\n",
       "    <pre>Main Loop  13%|<span class=\"enlighten-fg-red\">██▍               </span>|  1301/10000 [3h 46:15&lt;1d 1h 12:45, 0.10 epochs/s] loss=1.3427e-01</pre>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import enlighten\n",
    "\n",
    "\n",
    "# Training configuration parameters\n",
    "maxepochs  = tr_config[\"maxiter_adam\"]  # Maximum number of epochs for training with Adam optimizer\n",
    "lr0        = tr_config[\"lr_0\"]          # Initial learning rate for Adam optimizer\n",
    "decay_rate = tr_config[\"lr_gamma\"]      # Learning rate decay factor\n",
    "decay_step = tr_config[\"lr_step\"]       # Number of steps after which the learning rate is decayed\n",
    "lrf        = tr_config[\"lr_f\"]          # Final learning rate (minimum after decay)\n",
    "\n",
    "# Progress bar setup using enlighten for visual feedback during training\n",
    "manager = enlighten.get_manager()  # Initialize the progress manager\n",
    "outer_bar_format = u'{desc}{desc_pad}{percentage:3.0f}%|{bar}| ' + \\\n",
    "                 u'{count:{len_total}d}/{total:d} ' + \\\n",
    "                 u'[{elapsed}<{eta}, {rate:.2f}{unit_pad}{unit}/s] ' + \\\n",
    "                 u'{loss}{current_loss:1.4e}'  # Custom format for the progress bar, showing loss and time details\n",
    "\n",
    "# Create the progress bar for the outer loop (training epochs)\n",
    "pbar_outer = manager.counter(\n",
    "    total=maxepochs,               # Total number of epochs (training iterations)\n",
    "    desc=\"Main Loop\",              # Description of the progress bar\n",
    "    unit=\"epochs\",                 # Unit to track (epochs)\n",
    "    color=\"red\",                   # Color of the progress bar\n",
    "    loss=\"loss=\",                  # Prefix for displaying the loss value\n",
    "    current_loss=1e+9,             # Initial high loss value for display\n",
    "    bar_format=outer_bar_format    # Format defined above for the progress bar\n",
    ")\n",
    "\n",
    "# Optimizer setup using Optax (Adam optimizer with exponential learning rate decay)\n",
    "# Learning rate decays over time with the given decay rate and final value\n",
    "opt_adam = optax.adam(optax.exponential_decay(lr0, decay_step, decay_rate, end_value=lrf))\n",
    "\n",
    "# Initialize training state\n",
    "class TrainState(train_state.TrainState):\n",
    "    loss_fn = staticmethod(loss_fn)\n",
    "\n",
    "tx = opt_adam\n",
    "state = TrainState.create(apply_fn=model.apply, params=variables['params'], tx=tx)\n",
    "\n",
    "# Initialize lists to log loss values and minimum loss over epochs\n",
    "log_loss  = []  # Track the loss value for each epoch\n",
    "log_minloss = []  # Track the minimum loss value observed so far\n",
    "\n",
    "# Main training loop for the specified number of epochs\n",
    "for epoch in range(maxepochs):\n",
    "    batch_loss = 0.0  # Track cumulative loss over the batches in each epoch\n",
    "    num_batches = len(dataset) // batch_size  # Assuming dataset length is known\n",
    "    \n",
    "    for batch_signal, batch_coefficients in data_loader(dataset, batch_size):\n",
    "        # Generate a new PRNG key for dropout for this batch\n",
    "        rng_key, subkey = jax.random.split(rng_key)\n",
    "\n",
    "        # Compute loss and gradients\n",
    "        loss, grads = jax.value_and_grad(state.loss_fn)(\n",
    "            state.params, model, batch_signal, batch_coefficients, deterministic=False, rng_key=subkey\n",
    "        )\n",
    "        \n",
    "        # Apply gradients to update model parameters\n",
    "        state = state.apply_gradients(grads=grads)\n",
    "        \n",
    "        # Accumulate batch loss\n",
    "        batch_loss += loss\n",
    "\n",
    "    # Average batch loss for the epoch\n",
    "    avg_epoch_loss = batch_loss / num_batches\n",
    "\n",
    "    # Update the progress bar with the current average loss\n",
    "    pbar_outer.update(current_loss=avg_epoch_loss)\n",
    "    \n",
    "    # Log the loss for this epoch\n",
    "    log_loss.append(avg_epoch_loss)\n",
    "    \n",
    "    # Initialize or update the minimum loss and save the parameters\n",
    "    if epoch == 0 or avg_epoch_loss < minloss:\n",
    "        minloss = avg_epoch_loss\n",
    "        params_opt = state.params  # Save the parameters with the lowest loss\n",
    "    \n",
    "    log_minloss.append(minloss)\n",
    "\n",
    "# Stop the progress bar once training is complete\n",
    "manager.stop()\n",
    "\n",
    "# Optionally, print or save the logs\n",
    "print(f\"Training completed. Final loss: {log_loss[-1]}, Minimum loss: {minloss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the best model parameters\n",
    "with open('model_params.pkl', 'wb') as f:\n",
    "    pickle.dump(params_opt, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "houtlaw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
