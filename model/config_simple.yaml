seed: 0

gpu_id: 0  # GPU ID to use for training

paths:
  label_data_file_path: '/home/houtlaw/iono-net/data/baselines/10k_lownoise/train_compl_ampls_20250206_104902.csv'  # Path to label data file
  signal_data_file_path: '/home/houtlaw/iono-net/data/baselines/10k_lownoise/test_uscStruct_vals_20250206_104913.csv'

  test_label_file_path: '/home/houtlaw/iono-net/data/baselines/10k_lownoise/test_compl_ampls_20250206_104913.csv'
  test_data_file_path: '/home/houtlaw/iono-net/data/baselines/10k_lownoise/test_uscStruct_vals_20250206_104913.csv'

  x_range_file_path: '/home/houtlaw/iono-net/data/baselines/10k_lownoise/meta_X_20250206_104914.csv'
  setup_file_path: '/home/houtlaw/iono-net/data/baselines/10k_lownoise/setup_20250206_104914.json'
  kpsi_file_path: '/home/houtlaw/iono-net/data/baselines/10k_lownoise/kPsi_20250206_104914.csv'

model:
  architecture: [200,200,200,200,200,200,200]  # Model architecture
  activation: tanh  # Activation function (e.g., relu, tanh, sigmoid)

training:
  batch_size: 64  # Batch size
  gradient_clip_value: 0.0001 # Gradient clipping value
  l2_reg_weight: 0.3
  l4_freq: 10 # calculate l4 cost every 10 iterations
  l4_weight: 0.3 # weight for l4 cost

learning_rate:
  initial: 0.00003  # Initial learning rate
  gamma: 1.5  # Learning rate decay factor
  step: 20 # Step size for learning rate decay
  final: 0.00001  # Final learning rate

optimizer:
  maxiter_adam: 3000  # Maximum number of iterations for Adam optimizer
